image:
  tag: "476"
  repository: trinodb/trino
  pullPolicy: IfNotPresent

env:
  - name: AZURE_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: betting-fs-storage-secret
        key: account_key

service:
  type: LoadBalancer
  port: 8080
ingress:
  enabled: true
  className: webapprouting.kubernetes.azure.com
  hosts:
    - paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: trino
              port:
                number: 8080

server:
  workers: 1 # Number of worker nodes. Total nodes = 1 (coordinator) + 1 (workers) = 2
  config:
    query:
      maxMemory: "200GB" # Total user memory for a query across the cluster
      maxTotalMemory: "400GB" # Total memory (user + revocable) for a query across the cluster
  
catalogs:
  betting_warehouse: |-
    connector.name=iceberg
    iceberg.catalog.type=rest
    iceberg.rest-catalog.uri=http://nessie.default.svc.cluster.local:19120/iceberg
    fs.native-azure.enabled=true
    azure.auth-type=ACCESS_KEY
    azure.access-key=${ENV:AZURE_ACCESS_KEY}

  betting_warehouse_dev: |-
    connector.name=iceberg
    iceberg.catalog.type=rest
    iceberg.rest-catalog.uri=http://nessie.default.svc.cluster.local:19120/iceberg
    iceberg.rest-catalog.prefix=dev
    fs.native-azure.enabled=true
    azure.auth-type=ACCESS_KEY
    azure.access-key=${ENV:AZURE_ACCESS_KEY}

coordinator:
  jvm:
    maxHeapSize: "96G" 
    additionalJVMConfig:
      - "-XX:+ExitOnOutOfMemoryError"
      - "-XX:+HeapDumpOnOutOfMemoryError"
      - "-XX:HeapDumpPath=/var/log/trino/heapdump.hprof"
      - "-XX:+UseG1GC"
      - "-XX:G1HeapRegionSize=32M"
      - "-XX:G1MaxNewSizePercent=60"
      - "-XX:G1SurvivorRatio=10"
      - "-XX:G1HeapWastePercent=5"
      - "-XX:G1MixedGCPercentage=30"
      - "-XX:InitiatingHeapOccupancyPercent=35"
      - "-XX:OnOutOfMemoryError=kill -9 %p"
  
  config:
    node:
      environment: production      
    coordinator: true
    node-scheduler:
      include-coordinator: false 
    http-server:
      http:
        port: 8080
    discovery:
      uri: "http://trino:8080" 
    query:
      maxMemoryPerNode: "48GB"
    memory:
      heapHeadroomPerNode: "15GB" 
          
  nodeSelector:
      agentpool: agentpool
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: agentpool
                operator: In
                values:
                  - agentpool

worker:
  jvm:
    maxHeapSize: "96G"
    additionalJVMConfig:
      - "-XX:+ExitOnOutOfMemoryError"
      - "-XX:+HeapDumpOnOutOfMemoryError"
      - "-XX:HeapDumpPath=/var/log/trino/heapdump.hprof"
      - "-XX:+UseG1GC"
      - "-XX:G1HeapRegionSize=32M"
      - "-XX:G1MaxNewSizePercent=60"
      - "-XX:G1SurvivorRatio=10"
      - "-XX:G1HeapWastePercent=5"
      - "-XX:G1MixedGCPercentage=30"
      - "-XX:InitiatingHeapOccupancyPercent=35"
      - "-XX:OnOutOfMemoryError=kill -9 %p"
  config:
    node:
      environment: production
    memory:
      heapHeadroomPerNode: "15GB"
    query:
      maxMemoryPerNode: "48GB"
      
  autoscaling:
      enabled: true
      minReplicas: 0         # Min number of worker pods
      maxReplicas: 4         # Max number of worker pods
      targetCPUUtilizationPercentage: 70   # Scale up/down based on CPU usage
      targetMemoryUtilizationPercentage: 75 # Also scale based on memory usage
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max
          
  nodeSelector:
      agentpool: workerpool
  tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "worker"
        effect: "NoSchedule"
      - key: "kubernetes.azure.com/scalesetpriority"
        operator: "Equal"
        value: "spot"
        effect: "NoSchedule"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: agentpool
                operator: In
                values:
                  - workerpool
  resources:
    requests:
      memory: "100Gi" # Slightly more than 96G -Xmx
      cpu: "10" 
    limits:
      memory: "105Gi" # Should be larger than request and -Xmx
      cpu: "14"
